{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spracherkennung\n",
    "\n",
    "Wenn wir irgendetwas mit Texten machen wollen, ist es fast immer notwendig zu wissen, in welcher Sprache die texte geschrieben sind. Egal ob wir eine Rechtschreibkorrektur machen, einen Suchindex aufbauen oder Personennamen im Text finden wollen, die Verarbeitung der Texte ist sprachspezifisch. Manchmal gibt es Metadaten zu Texten, in denen die Sprache genannt wird. Diese sind aber oft nicht korrekt und meistens haben wir übehaupt keine Sprachangaben für die Texte, mit denen wir etwas machen wollen. In vielen Fällen ist es aber einfach, die Sprache des Textes automatisch zu erkennen.\n",
    "\n",
    "\n",
    "## Problemdefinition\n",
    "\n",
    "Was ist die genaue Aufgabe?\n",
    "\n",
    "### Anzahl der Sprachen\n",
    "\n",
    "Wenn wir aus zwei möglichen Sprachen wählen müssen, ist die Aufgabe einfacher als wenn wir aus 3000 Sprachen wählen müssen. In vielen praktischen Fällen muss man tatsächlich nur aus einigen Sprachen wählen. Wenn wir z.B. Social Media zu einem Deutschen Thema oder von einer deutschen Seite aus sammeln, können wir davon ausgehen, dass 95% aller Berichte deutsch oder englisch sind. Das gleiche gilt z.B. auch wenn wir Abstracts im Katalog einer deutschen Bibliothek analysieren.\n",
    "\n",
    "Im folgenden werden wir versuchen aus einer kleinen Anzahl West-Europäischer Sprachen zu wählen.\n",
    "\n",
    "### Textkodierung\n",
    "\n",
    "Im schlimmsten Fall haben wir eine Datei von der wir nicht mal wissen, wie die Zeichen kodiert sind. Wir lesen also Bits und wissen nicht, ob wir die Bits nach der ASCII-,  UTF8-, Latin2-Tabelle usw. interpretieren müssen. Wir müssen jetzt sowohl die Sprache und die Kodierung erraten. Im folgenden vereinfachen wir das Problem, und gehen davon aus, dass alle Text, die wir bekommen in UTF8 kodiert sind.\n",
    "\n",
    "## Verfahren\n",
    "\n",
    "Ein einfaches Verfahren, dass man sich vorstellen könnte wäre folgendes:\n",
    "Wir schauen in einem Text, wie viele häufig vorkommende deutsche Wörter wir finden. Hierzu muss der Text aber erstens lang genug sein, um eine zuverlässige Aussagen über die Sprache machen zu können, da manche Wörter in verschiedenen Sprachen vorkommen. Eine in vielen Internetforen häufig vorgeschlagene Variante ist die, dass man Stopwörter (Stopwörter sind sehr häufig vorkommende Wörter ohne eigene Bedeutung, wie Artikel, Pronomina, usw.) zählt, das Stopwörter häufig vorkommen und Stopwortlisten für vielen Sprachen leicht erhältlich sind. Für kürzere Text funktioniert dieser Ansatz leider nicht. Wörter wie *de* und *en* kommen sowohl in niederländischen wir auch in französischen Stopwortlisten vor. Solche Doppelungen gibt es oft, das Stopwörter meist ein- oder zweisiblige Wörter sind, und die Zahl der möglichen einsilbigen Wörter natürlich nicht unbegrenzt ist. Ein Buchtitel wie \n",
    "\n",
    "*De kleine prins en de grote drakejacht*\n",
    "\n",
    "könnte nach dem Stopwortverfahren genau so gut niederländisch als französisch sein, während es für jemand, der beide Sprachen einigermaßen kennt, sofort klar ist, das es sich beim Beispiel um eine niederlänische Phrase handelt.\n",
    "\n",
    "Bessere Ergebnisse bekommen wir, wenn wir uns nicht auf Wörter konzentrieren, sondern uns die Verteilung von Buchstaben anschauen. Im Englischen gibt es häufig einen *y* im Deutschen finden wir viele Buchstaben mit Umlauten usw. Noch besser funktionieren kombinationen von 2 und 3 Buchstaben, sogenannt Bi- und Trigramme. Wir können sogar einzelne Buchstaben, Bi- und Trigramme kombinieren. Dieser Ansatz ist beschrieben im folgenden Paper:\n",
    "\n",
    "Cavnar, W.B., Trenkle, J.M.: *N-gram-based text categorization*. In: Proceedings\n",
    "of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information\n",
    "Retrieval. pp. 161-175 (1994)\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.3248&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrahieren der n-Gramme\n",
    "\n",
    "Folgende Funktion extrahiert n-Gramme aus einem String:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.166539Z",
     "start_time": "2017-09-04T14:24:14.159538Z"
    }
   },
   "outputs": [],
   "source": [
    "def ngram(string,n):\n",
    "    liste = []\n",
    "    if n < len(string):\n",
    "        for p in range(len(string) - n + 1) :\n",
    "            tg = string[p:p+n]\n",
    "            liste.append(tg)\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir testen die Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.184540Z",
     "start_time": "2017-09-04T14:24:14.168539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Die', 'ie ', 'e V', ' Ve', 'Ver', 'erf', 'rfa', 'fas', 'ass', 'sse', 'ser', 'eri', 'rin', 'in ', 'n u', ' un', 'unt', 'nte', 'ter', 'ern', 'rni', 'nim', 'imm', 'mmt', 'mt ', 't e', ' es', 'es ', 's i', ' in', 'in ', 'n d', ' di', 'die', 'ies', 'ese', 'sem', 'em ', 'm B', ' Bu', 'Buc', 'uch', 'che', 'he,', 'e, ', ', d', ' di', 'die', 'ie ', 'e G', ' Ge', 'Ges', 'esc', 'sch', 'chi', 'hic', 'ich', 'cht', 'hte', 'te ', 'e d', ' de', 'des', 'es ', 's K', ' Ka', 'Kau', 'aut', 'uts', 'tsc', 'sch', 'chu', 'huk', 'uks', 'ks ', 's i', ' in', 'in ', 'n M', ' Me', 'Men', 'ens', 'nsc', 'sch', 'che', 'hen', 'ens', 'nsc', 'sch', 'chi', 'hic', 'ick', 'cks', 'ksa', 'sal', 'ale', 'len', 'en ', 'n z', ' zu', 'zu ', 'u e', ' er', 'erz', 'rzä', 'zäh', 'ähl', 'hle', 'len', 'en.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Die Verfasserin unternimmt es in diesem Buche, die Geschichte des Kautschuks in Menschenschicksalen zu erzählen.\"\n",
    "trigramme = ngram(text,3)\n",
    "print(trigramme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laut Cavnar e.a. bekommen wir die beste Ergebnisse, wenn wir Mono-, Bi- und Trigramme im Kombination nutzen. Wir schreiben also eine Funktion, die alle n-Gramme, für $1 \\leq n < 4$ extrahiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.198542Z",
     "start_time": "2017-09-04T14:24:14.194541Z"
    }
   },
   "outputs": [],
   "source": [
    "def xgram(string):\n",
    "    return [w for n in range(1,4) for w in ngram(string.lower(),n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.206543Z",
     "start_time": "2017-09-04T14:24:14.200542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'i', 'e', ' ', 'v', 'e', 'r', 'f', 'a', 's', 's', 'e', 'r', 'i', 'n', ' ', 'u', 'n', 't', 'e', 'r', 'n', 'i', 'm', 'm', 't', ' ', 'e', 's', ' ', 'i', 'n', ' ', 'd', 'i', 'e', 's', 'e', 'm', ' ', 'b', 'u', 'c', 'h', 'e', ',', ' ', 'd', 'i', 'e', ' ', 'g', 'e', 's', 'c', 'h', 'i', 'c', 'h', 't', 'e', ' ', 'd', 'e', 's', ' ', 'k', 'a', 'u', 't', 's', 'c', 'h', 'u', 'k', 's', ' ', 'i', 'n', ' ', 'm', 'e', 'n', 's', 'c', 'h', 'e', 'n', 's', 'c', 'h', 'i', 'c', 'k', 's', 'a', 'l', 'e', 'n', ' ', 'z', 'u', ' ', 'e', 'r', 'z', 'ä', 'h', 'l', 'e', 'n', '.', 'di', 'ie', 'e ', ' v', 've', 'er', 'rf', 'fa', 'as', 'ss', 'se', 'er', 'ri', 'in', 'n ', ' u', 'un', 'nt', 'te', 'er', 'rn', 'ni', 'im', 'mm', 'mt', 't ', ' e', 'es', 's ', ' i', 'in', 'n ', ' d', 'di', 'ie', 'es', 'se', 'em', 'm ', ' b', 'bu', 'uc', 'ch', 'he', 'e,', ', ', ' d', 'di', 'ie', 'e ', ' g', 'ge', 'es', 'sc', 'ch', 'hi', 'ic', 'ch', 'ht', 'te', 'e ', ' d', 'de', 'es', 's ', ' k', 'ka', 'au', 'ut', 'ts', 'sc', 'ch', 'hu', 'uk', 'ks', 's ', ' i', 'in', 'n ', ' m', 'me', 'en', 'ns', 'sc', 'ch', 'he', 'en', 'ns', 'sc', 'ch', 'hi', 'ic', 'ck', 'ks', 'sa', 'al', 'le', 'en', 'n ', ' z', 'zu', 'u ', ' e', 'er', 'rz', 'zä', 'äh', 'hl', 'le', 'en', 'n.', 'die', 'ie ', 'e v', ' ve', 'ver', 'erf', 'rfa', 'fas', 'ass', 'sse', 'ser', 'eri', 'rin', 'in ', 'n u', ' un', 'unt', 'nte', 'ter', 'ern', 'rni', 'nim', 'imm', 'mmt', 'mt ', 't e', ' es', 'es ', 's i', ' in', 'in ', 'n d', ' di', 'die', 'ies', 'ese', 'sem', 'em ', 'm b', ' bu', 'buc', 'uch', 'che', 'he,', 'e, ', ', d', ' di', 'die', 'ie ', 'e g', ' ge', 'ges', 'esc', 'sch', 'chi', 'hic', 'ich', 'cht', 'hte', 'te ', 'e d', ' de', 'des', 'es ', 's k', ' ka', 'kau', 'aut', 'uts', 'tsc', 'sch', 'chu', 'huk', 'uks', 'ks ', 's i', ' in', 'in ', 'n m', ' me', 'men', 'ens', 'nsc', 'sch', 'che', 'hen', 'ens', 'nsc', 'sch', 'chi', 'hic', 'ick', 'cks', 'ksa', 'sal', 'ale', 'len', 'en ', 'n z', ' zu', 'zu ', 'u e', ' er', 'erz', 'rzä', 'zäh', 'ähl', 'hle', 'len', 'en.']\n"
     ]
    }
   ],
   "source": [
    "xgramme = xgram(text)\n",
    "print(xgramme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprachmodelle\n",
    "\n",
    "Ein **Modell** für eine Sprache ist jetzt eine Menge solcher n-Gramme, mit dazu jeweils die Wahrscheinlichkeit, dass diese n-Gramme vorkommen. Ein Modell ist in Python also ein Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.218544Z",
     "start_time": "2017-09-04T14:24:14.208543Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildmodel(text):\n",
    "    model = {}\n",
    "\n",
    "    xgramme = xgram(text)\n",
    "    nr_of_ngs = len(xgramme)\n",
    "\n",
    "    for w in xgramme:\n",
    "        f = 1 + model.get(w,0)\n",
    "        model[w] = f\n",
    "    \n",
    "    for w in model:\n",
    "        model[w] = float(model[w]) / float(nr_of_ngs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir testen die Funktion und lassen uns das Ergebnis anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.237546Z",
     "start_time": "2017-09-04T14:24:14.231545Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def buildmodel(text):\n",
    "    model = collections.Counter(xgram(text))  \n",
    "    nr_of_ngs = sum(model.values())\n",
    "\n",
    "    for w in model:\n",
    "        model[w] = float(model[w]) / float(nr_of_ngs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:14.246547Z",
     "start_time": "2017-09-04T14:24:14.240546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 0.05105105105105105, ' ': 0.042042042042042045, 's': 0.03303303303303303, 'i': 0.02702702702702703, 'n': 0.02702702702702703, 'c': 0.021021021021021023, 'h': 0.021021021021021023, 'ch': 0.018018018018018018, 'u': 0.015015015015015015, 'd': 0.012012012012012012, 'r': 0.012012012012012012, 't': 0.012012012012012012, 'm': 0.012012012012012012, 'er': 0.012012012012012012, 'n ': 0.012012012012012012, 'es': 0.012012012012012012, 'sc': 0.012012012012012012, 'en': 0.012012012012012012, 'sch': 0.012012012012012012, 'a': 0.009009009009009009, 'k': 0.009009009009009009, 'di': 0.009009009009009009, 'ie': 0.009009009009009009, 'e ': 0.009009009009009009, 'in': 0.009009009009009009, 's ': 0.009009009009009009, ' d': 0.009009009009009009, 'die': 0.009009009009009009, 'in ': 0.009009009009009009, 'l': 0.006006006006006006, 'z': 0.006006006006006006, 'se': 0.006006006006006006, 'te': 0.006006006006006006, ' e': 0.006006006006006006, ' i': 0.006006006006006006, 'he': 0.006006006006006006, 'hi': 0.006006006006006006, 'ic': 0.006006006006006006, 'ks': 0.006006006006006006, 'ns': 0.006006006006006006, 'le': 0.006006006006006006, 'ie ': 0.006006006006006006, 'es ': 0.006006006006006006, 's i': 0.006006006006006006, ' in': 0.006006006006006006, ' di': 0.006006006006006006, 'che': 0.006006006006006006, 'chi': 0.006006006006006006, 'hic': 0.006006006006006006, 'ens': 0.006006006006006006, 'nsc': 0.006006006006006006, 'len': 0.006006006006006006, 'v': 0.003003003003003003, 'f': 0.003003003003003003, 'b': 0.003003003003003003, ',': 0.003003003003003003, 'g': 0.003003003003003003, 'ä': 0.003003003003003003, '.': 0.003003003003003003, ' v': 0.003003003003003003, 've': 0.003003003003003003, 'rf': 0.003003003003003003, 'fa': 0.003003003003003003, 'as': 0.003003003003003003, 'ss': 0.003003003003003003, 'ri': 0.003003003003003003, ' u': 0.003003003003003003, 'un': 0.003003003003003003, 'nt': 0.003003003003003003, 'rn': 0.003003003003003003, 'ni': 0.003003003003003003, 'im': 0.003003003003003003, 'mm': 0.003003003003003003, 'mt': 0.003003003003003003, 't ': 0.003003003003003003, 'em': 0.003003003003003003, 'm ': 0.003003003003003003, ' b': 0.003003003003003003, 'bu': 0.003003003003003003, 'uc': 0.003003003003003003, 'e,': 0.003003003003003003, ', ': 0.003003003003003003, ' g': 0.003003003003003003, 'ge': 0.003003003003003003, 'ht': 0.003003003003003003, 'de': 0.003003003003003003, ' k': 0.003003003003003003, 'ka': 0.003003003003003003, 'au': 0.003003003003003003, 'ut': 0.003003003003003003, 'ts': 0.003003003003003003, 'hu': 0.003003003003003003, 'uk': 0.003003003003003003, ' m': 0.003003003003003003, 'me': 0.003003003003003003, 'ck': 0.003003003003003003, 'sa': 0.003003003003003003, 'al': 0.003003003003003003, ' z': 0.003003003003003003, 'zu': 0.003003003003003003, 'u ': 0.003003003003003003, 'rz': 0.003003003003003003, 'zä': 0.003003003003003003, 'äh': 0.003003003003003003, 'hl': 0.003003003003003003, 'n.': 0.003003003003003003, 'e v': 0.003003003003003003, ' ve': 0.003003003003003003, 'ver': 0.003003003003003003, 'erf': 0.003003003003003003, 'rfa': 0.003003003003003003, 'fas': 0.003003003003003003, 'ass': 0.003003003003003003, 'sse': 0.003003003003003003, 'ser': 0.003003003003003003, 'eri': 0.003003003003003003, 'rin': 0.003003003003003003, 'n u': 0.003003003003003003, ' un': 0.003003003003003003, 'unt': 0.003003003003003003, 'nte': 0.003003003003003003, 'ter': 0.003003003003003003, 'ern': 0.003003003003003003, 'rni': 0.003003003003003003, 'nim': 0.003003003003003003, 'imm': 0.003003003003003003, 'mmt': 0.003003003003003003, 'mt ': 0.003003003003003003, 't e': 0.003003003003003003, ' es': 0.003003003003003003, 'n d': 0.003003003003003003, 'ies': 0.003003003003003003, 'ese': 0.003003003003003003, 'sem': 0.003003003003003003, 'em ': 0.003003003003003003, 'm b': 0.003003003003003003, ' bu': 0.003003003003003003, 'buc': 0.003003003003003003, 'uch': 0.003003003003003003, 'he,': 0.003003003003003003, 'e, ': 0.003003003003003003, ', d': 0.003003003003003003, 'e g': 0.003003003003003003, ' ge': 0.003003003003003003, 'ges': 0.003003003003003003, 'esc': 0.003003003003003003, 'ich': 0.003003003003003003, 'cht': 0.003003003003003003, 'hte': 0.003003003003003003, 'te ': 0.003003003003003003, 'e d': 0.003003003003003003, ' de': 0.003003003003003003, 'des': 0.003003003003003003, 's k': 0.003003003003003003, ' ka': 0.003003003003003003, 'kau': 0.003003003003003003, 'aut': 0.003003003003003003, 'uts': 0.003003003003003003, 'tsc': 0.003003003003003003, 'chu': 0.003003003003003003, 'huk': 0.003003003003003003, 'uks': 0.003003003003003003, 'ks ': 0.003003003003003003, 'n m': 0.003003003003003003, ' me': 0.003003003003003003, 'men': 0.003003003003003003, 'hen': 0.003003003003003003, 'ick': 0.003003003003003003, 'cks': 0.003003003003003003, 'ksa': 0.003003003003003003, 'sal': 0.003003003003003003, 'ale': 0.003003003003003003, 'en ': 0.003003003003003003, 'n z': 0.003003003003003003, ' zu': 0.003003003003003003, 'zu ': 0.003003003003003003, 'u e': 0.003003003003003003, ' er': 0.003003003003003003, 'erz': 0.003003003003003003, 'rzä': 0.003003003003003003, 'zäh': 0.003003003003003003, 'ähl': 0.003003003003003003, 'hle': 0.003003003003003003, 'en.': 0.003003003003003003})\n"
     ]
    }
   ],
   "source": [
    "model = buildmodel(text)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir jetzt ausreichend Text haben, bekommen wir typische Werte für eine Sprache. Die Werte in einem unbekannten Text können wir hiermit vergleichen.\n",
    "\n",
    "Im NLTK-Paket ist die Erklärung der Menschenrechte in 300 Sprachen enthalten. Hiermit können wir jetzt mal einige Sprachmodelle bauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie das Paket nltk zum ersten mal nutzen, oder bisher keine Corpora benutzt haben, bekommen Sie vielleicht eine Fehlermeldung, die darauf zurückgeführt werden kann, dass nicht alle Ressourcen aus dem Paket installiert sind. Sie können in dem Fall einmalig den folgenden Code ausführen. Es erscheint jetzt ein Fenster, in dem Sie auswählen können, welche Ressourcen Sie herunterladen möchte. Die Option 'Book' ist für uns ausreichend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.191041Z",
     "start_time": "2017-09-04T14:24:14.248547Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "\n",
    "#print(udhr.fileids())\n",
    "\n",
    "languages = ['english','german','dutch','portuguese','spanish','turkish']\n",
    "\n",
    "english_udhr = udhr.raw('English-Latin1')\n",
    "german_udhr = udhr.raw('German_Deutsch-Latin1')\n",
    "dutch_udhr = udhr.raw('Dutch_Nederlands-Latin1')\n",
    "portuguese_udhr = udhr.raw('Portuguese_Portugues-Latin1')\n",
    "spanish_udhr = udhr.raw('Spanish_Espanol-Latin1')\n",
    "turkish_udhr = udhr.raw('Turkish_Turkce-Turkish')\n",
    "\n",
    "texts = {'english':english_udhr,'german':german_udhr,'dutch':dutch_udhr,'portuguese':portuguese_udhr,'turkish':turkish_udhr,'spanish':spanish_udhr}\n",
    "models = {lang:buildmodel(texts[lang]) for lang in languages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Texte sind sehr kurz und entalten nur ein Bruchteil der Wörter, die in einer Sprache vorkommen. Fürs Deutsche haben wir gerade mal 10 000 Zeichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.198042Z",
     "start_time": "2017-09-04T14:24:19.193041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "print(len(german_udhr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprache Bestimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssen jetzt die n-Gram Frequenzen eiens Textes mit den Frequenzen der Modelle vergleichen. Um die Modelle zu vergleichen berechnen wir den Cosinus:\n",
    "$$\n",
    "cos(a,b) = \\frac{\\sum_i a_i b_i}{\\sqrt{\\sum_i a_i^2}\\sqrt{\\sum_i b_i^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.211043Z",
     "start_time": "2017-09-04T14:24:19.200042Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosinus(a,b):\n",
    "    return sum([a[k]*b[k] for k in a if k in b]) / (math.sqrt(sum([a[k]**2 for k in a])) * math.sqrt(sum([b[k]**2 for k in b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.229045Z",
     "start_time": "2017-09-04T14:24:19.213043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Verfasserin unternimmt es in diesem Buche, die Geschichte des Kautschuks in Menschenschicksalen zu erzählen.\n",
      "english 0.7411160989023053\n",
      "german 0.8522092567280237\n",
      "dutch 0.7916873428505355\n",
      "portuguese 0.7129068566754563\n",
      "spanish 0.7444444064683035\n",
      "turkish 0.730643500627457\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "textmodel = buildmodel(text)\n",
    "for m in models:\n",
    "    print(m, cosinus(models[m],textmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass es jetzt schon funktioniert! Zuverlässiger wird es natürlich, wenn wir längere Texte und mehr Genres nehmen um die Sprachmodelle zu bauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein wenig Kosmetik\n",
    "\n",
    "Wir brauchen schließlich eine Funktion, die einfach sagt, in welcher Sprache ein Text geschrieben ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.237046Z",
     "start_time": "2017-09-04T14:24:19.231045Z"
    }
   },
   "outputs": [],
   "source": [
    "def guess_language(text):\n",
    "    textmodel = buildmodel(text)\n",
    "    lang = \"english\"\n",
    "    best = 0\n",
    "    for m in models:\n",
    "        c = cosinus(models[m],textmodel)\n",
    "        if c > best:\n",
    "            best = c\n",
    "            lang = m\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.278050Z",
     "start_time": "2017-09-04T14:24:19.240046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dutch\n",
      "turkish\n",
      "portuguese\n"
     ]
    }
   ],
   "source": [
    "t = \"In een container die was geladen met medische hulpmiddelen zaten 395 pakketten cocaïne verstopt.\"  \n",
    "print(guess_language(t))\n",
    "\n",
    "t = \"Ağız maskesi nerelerde zorunlu? Toplu kullanıma açık iç mekanlarda (örneğin mağaza, süpermarket, müze, kütüphane, sinema, tiyatro, restoran, sahne, belediyeler, benzin istasyonları).\"  \n",
    "print(guess_language(t))\n",
    "\n",
    "\n",
    "t = \"O Reino Unido vai tornar-se no primeiro país do mundo a começar a vacinar a população contra a covid-19.\"\n",
    "print(guess_language(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei dieser Sprachauswahl reicht eigentlich schon fast ein einziger Buchstabe:\n",
    "* ı --> Türkisch\n",
    "* ã --> Portugiesisch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließlich wollen wir die Modelle nicht jedes mal neu berechnen. Wir speichern die Modelle daher in eine Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.291051Z",
     "start_time": "2017-09-04T14:24:19.280050Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(models, open('langidmodels.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die Modelle jetzt auch wieder einlesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.304052Z",
     "start_time": "2017-09-04T14:24:19.295051Z"
    }
   },
   "outputs": [],
   "source": [
    "models = pickle.load(open('langidmodels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungen\n",
    "\n",
    "1. Testen Sie den Classifier für verschiedene kurze und längere Texte\n",
    "2. Fügen Sie weitere Sprachen (auch Sprachen, die den bereits vorhandenen Sprachen sehr ähnlich sind, wie Luxemburgisch oder Katelanisch) hinzu, und testen Sie erneut.\n",
    "3. Suchen Sie größere Textcorpora zum Trainieren. Hier finden Sie eine Liste mit Corpora, die in NLTK verfügbar sind: http://www.nltk.org/book/ch02.html  Als deutsches Corpus können Sie das Tiger-Corpus nutzen. Das Corpus kann  von dieser Seite heruntergeladen werden: http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.html\n",
    "\n",
    "Sie können dieses Corpus wie folgt einlesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:19.312053Z",
     "start_time": "2017-09-04T14:24:19.306052Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "root = 'Corpora'\n",
    "fileid = 'tiger.16012013.conll09'\n",
    "columntypes = ['ignore','words','ignore','ignore','pos']\n",
    "\n",
    "tiger_corpus = nltk.corpus.ConllCorpusReader(root,fileid,columntypes,encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können eine Liste von Wörtern bekommen und müssen jetzt entweder hieraus einen Text basteln, oder das Modell aus einer Liste von Wörtern aufbauen. Es ist dann sinnvoll jedes Wort mit einem Wortanfang- und Wortendesymbol zu erweitern. Aus Diktator wird also *#Diktator#*. Sie haben dann auch  Bigramme, wie *#D* und *r#*. Daraus sieht man, dass ein Deutsches Wort mit D anfangen und auf r enden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T14:24:33.481470Z",
     "start_time": "2017-09-04T14:24:19.315053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888238\n",
      "['``', 'Ross', 'Perot', 'wäre', 'vielleicht', 'ein', 'prächtiger', 'Diktator', \"''\", 'Konzernchefs']\n"
     ]
    }
   ],
   "source": [
    "text_german = tiger_corpus.words()\n",
    "print(len(text_german))\n",
    "print(text_german[:10])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
